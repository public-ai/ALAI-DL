{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ╔══<i><b>Alai-DeepLearning</b></i>════════════════════════════╗\n",
    "###  &nbsp;&nbsp; **✎&nbsp;&nbsp;Week 6. MLP Basis**\n",
    "# Section 5. BackPropagation Overview\n",
    "\n",
    "### _Objective_\n",
    "1. 신경망을 학습시키는 가장 기본적인 원리인 BackPropagation에 대해 배워보겠습니다. <br>\n",
    "  \n",
    "╚═════════════════════════════════════════╝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "tf.random.set_random_seed(252)\n",
    "np.random.seed(100)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# \\[ 1. 역전파의 개괄 \\]\n",
    "\n",
    "----\n",
    "\n",
    "----\n",
    "\n",
    "> 심층 신경망은 기본적으로 입력 $x$를 받아서 출력 $\\hat y$를 산출합니다. 정보는 신경망을 따라 앞으로 흘러가는 이 과정을 이전까지 배웠듯이, **FeedForward(순전파)**라고 합니다.<br>\n",
    "\n",
    "> 훈련 과정에서는 순전파를 통해 계산된 손실 정보를 신경망을 따라 거꾸로(역방향으로) 흐르게 해서 기울기를 계산하는 과정입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 신경망의 학습 알고리즘\n",
    "---\n",
    "\n",
    "* 기본적으로 신경망의 학습은 선형 회귀, 로지스틱 회귀와 마찬가지로 Gradient Descent 알고리즘을 이용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Gradient Descent 알고리즘 복습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Imgur](https://i.imgur.com/ks7q74o.png)\n",
    "$$\n",
    "\\mbox{gradient descent 수식 }\\\\\n",
    "x_{i+1} = x_i - \\alpha * \\frac{\\partial f}{\\partial x}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graident Descent 방법으로 Weight를 갱신하기 위해서는<br>\n",
    "우리는 우리는 각 **Weight별 기울기**를 구해야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 심층 신경망에서의 Gradient Descent\n",
    "\n",
    "예제로 아래와 같이 2층 신경망 있다고 생각해봅시다.\n",
    "![Imgur](https://i.imgur.com/qRH21Ws.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같은 모델을 Gradient Descent 알고리즘으로 학습하고자 합니다.<br>\n",
    "\n",
    "* 첫번째 은닉층의 weight, bias update\n",
    "$$\n",
    "W^{[1]}_{new} = W^{[1]}_{old} - \\alpha*\\frac{\\partial Loss}{\\partial W^{[1]}}\\\\\n",
    "b^{[1]}_{new} = b^{[1]}_{old} - \\alpha*\\frac{\\partial Loss}{\\partial b^{[1]}}\n",
    "$$\n",
    "\n",
    "* 두번째 은닉층의 weight, bias update\n",
    "$$\n",
    "W^{[2]}_{new} = W^{[2]}_{old} - \\alpha*\\frac{\\partial Loss}{\\partial W^{[2]}}\\\\\n",
    "b^{[2]}_{new} = b^{[2]}_{old} - \\alpha*\\frac{\\partial Loss}{\\partial b^{[2]}}\n",
    "$$\n",
    "\n",
    "* 출력층의 weight, bias update\n",
    "$$\n",
    "W^{[3]}_{new} = W^{[3]}_{old} - \\alpha*\\frac{\\partial Loss}{\\partial W^{[3]}}\\\\\n",
    "b^{[3]}_{new} = b^{[3]}_{old} - \\alpha*\\frac{\\partial Loss}{\\partial b^{[3]}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리는 Gradient Descent 알고리즘을 적용하기 위해서<br>\n",
    "총 6 종류의 Gradient\n",
    "($\n",
    "\\frac{\\partial Loss}{\\partial W^{[1]}},\n",
    "\\frac{\\partial Loss}{\\partial b^{[1]}},\n",
    "\\frac{\\partial Loss}{\\partial W^{[2]}},\n",
    "\\frac{\\partial Loss}{\\partial b^{[2]}},\n",
    "\\frac{\\partial Loss}{\\partial W^{[3]}},\n",
    "\\frac{\\partial Loss}{\\partial b^{[3]}}\n",
    "$)을 계산해야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2. BackPropagation의 필요성\n",
    "----\n",
    "\n",
    "* BackPropagation은 심층 신경망 내 가중치들의 Gradient를 구하는 방법입니다.<br>\n",
    "* BackPropagation은 오차 수열의 점화식으로 구성되어 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) BackPropagation 없이 계산하기\n",
    "\n",
    "![Imgur](https://i.imgur.com/tlDSSKI.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 Weight Gradient 중 $W^{[1]}_{1,1}$을 계산하기 위해서는,<br>\n",
    "아래와 같이 장황한 Gradient 식이 필요합니다.\n",
    "\n",
    "![Imgur](https://i.imgur.com/qGUKEDY.png)\n",
    "\n",
    "깊이가 깊어질수록, Gradient 수식은 훨씬 더 장황해집니다.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) BackPropagation의 아이디어 : 점화식 패턴 이용하기\n",
    "\n",
    "오차 역전파법은 알고리즘 중 **Dynamic Programming**의 패턴입니다.<br>\n",
    "이미 계산된 부분을 저장하고, 이를 반복적으로 재적용해가는 식으로 이용됩니다.<br>\n",
    "\n",
    "![Imgur](https://i.imgur.com/CcAScJy.png)\n",
    "\n",
    "![Imgur](https://i.imgur.com/WQ5FnP4.png)\n",
    "\n",
    "![Imgur](https://i.imgur.com/l4tt4y3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 초록색으로 표시한 부분은 이전에 계산했던 부분입니다.<br>\n",
    "다시 계산하는 것이 아닌 이전에 계산한 부분을 재적용하면서,<br>\n",
    "Gradient를 재귀적으로 구해가는 과정을 **오차역전파법**이라고 부릅니다.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# \\[ 2. 미분 공식 복습하기 \\]\n",
    "\n",
    "----\n",
    "\n",
    "----\n",
    "\n",
    "> *Section 6~7은 미분에 대한 이해가 필수적입니다. 이를 위해, 주요 미분 공식을 복습하는 시간을 갖도록 하겠습니다.*<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## 1. 다항식 미분\n",
    "\n",
    "|$f(x)$   |$f'(x)$|\n",
    "|------|-------|\n",
    "|$c$|$0$|\n",
    "|$ax+b$|$a$|\n",
    "|$x^n$|$nx^{n-1}$|\n",
    "|$kx^n$|$nkx^{n-1}$|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) 예제 문제 \n",
    "\n",
    "> $f'(x)$를 구해주세요.\n",
    "\n",
    "\n",
    "$$\n",
    "f(x) = 2x^3+3x^2+5\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 예제 문제\n",
    "\n",
    "> $f'(x)$를 구해주세요.\n",
    "\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{2}{x^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 지수 로그 미분\n",
    "\n",
    "* 딥러닝에서 로그(log)는 기본적으로 자연로그(ln)로 취급합니다.\n",
    "\n",
    "|$$f(x)$$|$$f'(x)$$|\n",
    "|------|-------|\n",
    "|$$e^x$$|$$e^x$$|\n",
    "|$$a^x$$|$$a^xlog(a)$$|\n",
    "|$$logx$$|$$\\frac{1}{x}$$|\n",
    "|$$log_ax$$|$$\\frac{1}{x}\\frac{1}{log a}$$|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) 예제 문제 \n",
    "\n",
    "> $f'(x)$를 구해주세요.\n",
    "\n",
    "\n",
    "$$\n",
    "f(x) = 2e^{3x}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 예제 문제\n",
    "\n",
    "> $f'(x)$를 구해주세요.\n",
    "\n",
    "\n",
    "$$\n",
    "f(x) = 3log_5x\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 주요 미분 법칙\n",
    "\n",
    "1.  합의 미분<br>\n",
    "$\n",
    "y = f(x)+g(x) \\rightarrow y' = f'(x) + g'(x)\n",
    "$\n",
    "\n",
    "2. 곱의 미분<br>\n",
    "$\n",
    "y = f(x)g(x) \\rightarrow y' = f'(x)g(x) + g'(x)f(x)\n",
    "$\n",
    "\n",
    "3. 합성함수 미분<br>\n",
    "$\n",
    "y = f(g(x)) \\rightarrow y' = f'(g(x))g'(x)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) 예제 문제 \n",
    "\n",
    "> $f'(x)$를 구해주세요.\n",
    "\n",
    "\n",
    "$$\n",
    "f(x) = e^{3x} + log(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 예제 문제\n",
    "\n",
    "> $f'(x)$를 구해주세요.\n",
    "\n",
    "\n",
    "$$\n",
    "f(x) = xlog(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) 예제 문제\n",
    "\n",
    "> $f'(x)$를 구해주세요.\n",
    "\n",
    "\n",
    "$$\n",
    "f(x) = e^{3x^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    "\n",
    "---\n",
    "\n",
    "    Copyright(c) 2019 by Public AI. All rights reserved.<br>\n",
    "    Writen by PAI, SangJae Kang ( rocketgrowthsj@publicai.co.kr )  last updated on 2019/04/01\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
